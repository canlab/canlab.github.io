
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>demo_norming_comparison</title><meta name="generator" content="MATLAB 9.6"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2021-08-21"><meta name="DC.source" content="demo_norming_comparison.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">INTRODUCTION</a></li><li><a href="#2">Study descriptive statistics</a></li><li><a href="#3">Raw data</a></li><li><a href="#4">METHODS</a></li><li><a href="#5">RESULTS</a></li><li><a href="#6">MSE performance metrics</a></li><li><a href="#7">pearson-r performance metrics</a></li><li><a href="#8">evaluation time</a></li><li><a href="#9">CONCLUSIONS</a></li><li><a href="#10">NOTES</a></li></ul></div><h2 id="1">INTRODUCTION</h2><p>This page demonstrates how to use the (private) single trials dataset to formally compare algorithm performance, using the borderline trivial case of comparing performance under different rescaling procedures. It is simple to implement, and despite its conceptual simplicity is an unanswered question as of the time of this writing.</p><p>Single trial coefficients vary considerably across studies in systematic ways. Although different experimental conditions are expected to yeild different coefficient maps, there are considerable differences in the basic statistical features of these maps across studies which are more likely due to differences in statistical design than experimental design, and are thus unintended artifacts of the analysis procedure. In order to predict outcomes in one study based on models fit in another study we need to minimize variation due to statistical design choices, in particular spurious differenes in the statistical properties of these coefficients.</p><p>Additionally, parametric statistical maps in fMRI often use arbitrary units. Not only is the scale of outcome measures often arbitrary (for instance a pain rating scale might vary form 0-8 or 0-100. There may be anchors throughout the scale or only at the ends, it might be logarithmic or it might be linear; there are few conventions), but the preditor (the BOLD signal) is also not a reliable absolute measure (the situation might be different with something like arterial spin labeling). This suggests the scale of the parametric map (which is typically in unis of DV/IV) might be arbitrary.</p><p>Studies with similar task conditions often show differences in the magnitude (mean and variance) of coefficient estimates, leading to use of mean centering trials, l2-norming trials or z-scoring trials, in an attempt to correct for these artifacts. However, whether these scaling procedures remove task relevant biological information or not remains an open empirical question.</p><p>Here we use 5-fold cross validation in each of 15 datasets to establish whether within study predictive accuracy is improved or harmed by any of these three scaling procedures. The presumption is that within study the statistical design is invariant, and any change in performance due to these scaling procedures can therefore be attributed to a loss of sensitivity for biologically meaningful consequences of the experimental design. By examining 15 datasets we can informally differentiate variantion in performance due to chance, vs. systematic differences due to scaling choices, and thereby inform application of these scaling procedures to future MVPA decoding methods.</p><p>We use SVR as a method of MVPA based brain decoding, but similar results would be expected with many other linear machine learning algorithms (e.g. PCR). SVR is simply used for its performance advantage.</p><pre class="codeinput">close <span class="string">all</span>; clear <span class="string">all</span>;

t99 = tic;

warning(<span class="string">'off'</span>,<span class="string">'all'</span>);

addpath(genpath(<span class="string">'/dartfs-hpc/rc/home/m/f0042vm/software/canlab/CanlabCore/CanlabCore'</span>));
addpath(<span class="string">'/dartfs-hpc/rc/home/m/f0042vm/software/spm12'</span>);
addpath(genpath(<span class="string">'/dartfs-hpc/rc/home/m/f0042vm/software/canlab/canlab_single_trials'</span>)); <span class="comment">% single_trials repo</span>
addpath(genpath(<span class="string">'/dartfs-hpc/rc/home/m/f0042vm/software/canlab/canlab_single_trials_private'</span>)); <span class="comment">% single_trials private repo</span>
addpath(genpath(<span class="string">'/dartfs-hpc/rc/home/m/f0042vm/software/canlab/ooFmriDataObjML'</span>)); <span class="comment">% needed for cvpartition2</span>
addpath(<span class="string">'/dartfs/rc/lab/C/CANlab/labdata/projects/canlab_single_trials_for_git_repo'</span>); <span class="comment">% single trial data on blanca</span>

<span class="comment">% remove private repos from this list if you don't have access. Repos</span>
<span class="comment">% accessible to the public are listed on the canlab_single_trials github</span>
<span class="comment">% page and in its README.md file</span>
st_datasets = {<span class="string">'nsf'</span>,<span class="string">'bmrk3pain'</span>,<span class="string">'bmrk3warm'</span>,<span class="string">'bmrk4'</span>,<span class="string">'bmrk5pain'</span>,<span class="keyword">...</span>
        <span class="string">'bmrk5snd'</span>,<span class="string">'remi'</span>,<span class="string">'scebl'</span>,<span class="string">'ie2'</span>,<span class="string">'ie'</span>,<span class="string">'exp'</span>,<span class="string">'levoderm'</span>,<span class="string">'stephan'</span>,<span class="keyword">...</span>
        <span class="string">'romantic'</span>,<span class="string">'ilcp'</span>};

<span class="comment">% import studies, suppressing output to minimize clutter (it's long and</span>
<span class="comment">% redundant with more detailed information we provide below). This isn't</span>
<span class="comment">% available in the public repo.</span>
<span class="comment">% if you don't have access to the private repo you can substitute something</span>
<span class="comment">% like this instead (but you may need to debug, this is a general sketch):</span>
<span class="comment">% for i = 1:length(st_datasets)</span>
<span class="comment">%   dat{i} = load_image_set(st_datasets{i}));</span>
<span class="comment">%   dat{i}.metadata_table.study_id = i*ones(height(dat{i}.metadata_table),1);</span>
<span class="comment">% end</span>
<span class="comment">% all_dat = cat(dat{:}); clar dat;</span>
cmd = <span class="string">'all_dat = load_image_set(''all_single_trials'')'</span>;
fprintf(<span class="string">'cmd: %s\n'</span>,cmd);
evalc(cmd);

n_studies = length(st_datasets);
study_id = all_dat.metadata_table.study_id;
[uniq_study_id, ~, study_id] = unique(study_id,<span class="string">'rows'</span>,<span class="string">'stable'</span>);
</pre><pre class="codeoutput">cmd: all_dat = load_image_set('all_single_trials')
Warning: The 'rows' input is not supported for cell array inputs. 
</pre><h2 id="2">Study descriptive statistics</h2><p>notice how across studies each subject shows different mean (circle) and standard deviations (whiskers). Note the x-axis scales. For a model fit to one of these studies to make accurate prediction in a different study with a different \beta domain, the model would need to successfully extrapolate to that different \beta domain outside of its training space. This has very low probability of success, since our models are all linear approximations of the brain response, which is in all likelihood intrinsically nonlinear.</p><p>Thus, we have little hope of obtaining accurate absolute predictions in any other study if these \beta effects are task relevant and biologically meaningful. If they are artifacts of statistical design then it should be possible to approximately correct for them using rescaling procedures.</p><h2 id="3">Raw data</h2><pre class="codeinput">figure;
<span class="keyword">for</span> i = 1:n_studies
    this_idx = find(i == study_id);
    this_dat = all_dat.get_wh_image(this_idx);

    subject_id = this_dat.metadata_table.subject_id;
    [uniq_subject_id, ~, subject_id] = unique(subject_id,<span class="string">'rows'</span>,<span class="string">'stable'</span>);

    subplot(ceil(sqrt(n_studies)), ceil(n_studies/ceil(sqrt(n_studies))), i);
    hold <span class="string">off</span>
    <span class="keyword">for</span> j = 1:length(uniq_subject_id)
        subj_idx = j == subject_id;
        this_subj_dat = this_dat.dat(:,subj_idx);
        q(j,:) = quantile(this_subj_dat(:),[0.025,0.5,0.975]);
        mu = mean(mean(this_subj_dat(:)));
        sd = std(this_subj_dat(:));
        h1 = plot([mu-sd, mu+sd],[j,j],<span class="string">'-'</span>);
        hold <span class="string">on</span>;
        h2 = plot(mu,j,<span class="string">'o'</span>);
        h2.Color = h1.Color;
    <span class="keyword">end</span>
    box <span class="string">off</span>
    title([<span class="string">'Distribution of '</span>, uniq_study_id{i}]);
    xlabel(<span class="string">'\beta'</span>);
    ylabel(<span class="string">'Subj'</span>);
<span class="keyword">end</span>
p = get(gcf,<span class="string">'Position'</span>);
set(gcf,<span class="string">'Position'</span>,[p(1:2),1024,2048]);
</pre><pre class="codeoutput">Direct calls to spm_defauts are deprecated.
Please use spm('Defaults',modality) or spm_get_defaults instead.
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
Warning: The 'rows' input is not supported for cell array inputs. 
</pre><img vspace="5" hspace="5" src="demo_norming_comparison_01.png" alt=""> <h2 id="4">METHODS</h2><p>compute raw data, centered, l2normed and zscored images for continuous outcome prediction statistics. Fit SVR models to each, and compute 5-fold cross validated predictions of outcomes. See RESULTS comments for subsequent statistical design methodology.</p><pre class="codeinput"><span class="comment">% we don't have the memory to keep all studies in RAM at once for this next</span>
<span class="comment">% step, so we clear them and will process one study at a time</span>
clear <span class="string">all_dat</span>

n_studies = length(st_datasets);

[d_cverr, c_cverr, l2_cverr, z_cverr,<span class="keyword">...</span>
    d_t, c_t, l2_t, z_t] = deal(zeros(n_studies,1));
[d_stats, d_optout, <span class="keyword">...</span>
    c_stats, c_optout, <span class="keyword">...</span>
    l2_stats, l2_optout, <span class="keyword">...</span>
    z_stats, z_optout] = deal(cell(n_studies,1));

<span class="keyword">if</span> ~isempty(gcp(<span class="string">'nocreate'</span>))
    delete(gcp(<span class="string">'nocreate'</span>));
<span class="keyword">end</span>
parpool(4)
<span class="comment">% run each algorithm on each study and save results</span>
<span class="keyword">parfor</span> i = 1:n_studies
    warning(<span class="string">'off'</span>,<span class="string">'all'</span>);
    fprintf(<span class="string">'Evaluating study %s\n'</span>,st_datasets{i});
    this_study = st_datasets{i};

    this_dat = load_image_set(this_study,<span class="string">'verbose'</span>, 0);

    <span class="comment">% for completion datasets retain trials with invalid responses, so</span>
    <span class="comment">% remove them</span>
    this_dat = this_dat.get_wh_image(~isnan(this_dat.Y));
    <span class="comment">% this can be removed after dataset update</span>
    this_dat = this_dat.remove_empty;
    <span class="keyword">for</span> j = 1:size(this_dat.dat,2)
        this_dat.dat(isnan(this_dat.dat(:,j)),1) = 0;
    <span class="keyword">end</span>

    <span class="comment">% this is where you quartile the data if you want that</span>

    <span class="comment">% zscore outcome for more interpretable values of prediction MSE</span>
    <span class="comment">% and for easier comparison across studies (which use different outcome</span>
    <span class="comment">% scales otherwise)</span>
    this_dat.Y = zscore(this_dat.Y);

    <span class="comment">% manually select CV folds to</span>
    <span class="comment">%   a) ensure subjects aren't split across test folds (required for</span>
    <span class="comment">%       independence assumptions to be satisifed in CV)</span>
    <span class="comment">%   b) keep folds the same across algorithms (to reduce slicing</span>
    <span class="comment">%       related variance)</span>
    [~,~,subject_id] = unique(char(this_dat.metadata_table.subject_id),<span class="string">'rows'</span>,<span class="string">'stable'</span>);
    cv = cvpartition2(ones(size(this_dat.dat,2),1), <span class="string">'GroupKFold'</span>, 5, <span class="string">'Group'</span>, subject_id);
    fold_labels = zeros(size(this_dat.dat,2),1);
    <span class="keyword">for</span> j = 1:cv.NumTestSets
        fold_labels(cv.test(j)) = j;
    <span class="keyword">end</span>

    this_dat_c = this_dat.rescale(<span class="string">'centerimages'</span>);
    this_dat_l2 = this_dat.rescale(<span class="string">'l2norm_images'</span>);
    this_dat_z = this_dat.rescale(<span class="string">'zscoreimages'</span>);

    <span class="comment">% default</span>
    t0 = tic;
    [d_cverr(i), d_stats{i}, d_optout{i}] = predict(this_dat, <span class="string">'algorithm_name'</span>, <span class="string">'cv_svr'</span>, <span class="keyword">...</span>
        <span class="string">'nfolds'</span>, fold_labels, <span class="string">'error_type'</span>, <span class="string">'mse'</span>, <span class="string">'useparallel'</span>, 0, <span class="string">'verbose'</span>, 0);
    d_t(i) = toc(t0);

    <span class="comment">% centered</span>
    t0 = tic;
    [c_cverr(i), c_stats{i}, c_optout{i}] = predict(this_dat_c, <span class="string">'algorithm_name'</span>, <span class="string">'cv_svr'</span>, <span class="keyword">...</span>
        <span class="string">'nfolds'</span>, fold_labels, <span class="string">'error_type'</span>, <span class="string">'mse'</span>, <span class="string">'useparallel'</span>, 0, <span class="string">'verbose'</span>, 0);
    c_t(i) = toc(t0);

    <span class="comment">% l2normed</span>
    t0 = tic;
    [l2_cverr(i), l2_stats{i}, l2_optout{i}] = predict(this_dat_l2, <span class="string">'algorithm_name'</span>, <span class="string">'cv_svr'</span>, <span class="keyword">...</span>
        <span class="string">'nfolds'</span>, fold_labels, <span class="string">'error_type'</span>, <span class="string">'mse'</span>, <span class="string">'useparallel'</span>, 0, <span class="string">'verbose'</span>, 0);
    l2_t(i) = toc(t0);

    <span class="comment">% zscored</span>
    t0 = tic;
    [z_cverr(i), z_stats{i}, z_optout{i}] = predict(this_dat_z, <span class="string">'algorithm_name'</span>, <span class="string">'cv_svr'</span>, <span class="keyword">...</span>
        <span class="string">'nfolds'</span>, fold_labels, <span class="string">'error_type'</span>, <span class="string">'mse'</span>, <span class="string">'useparallel'</span>, 0, <span class="string">'verbose'</span>, 0);
    z_t(i) = toc(t0);
<span class="keyword">end</span>
</pre><pre class="codeoutput">Starting parallel pool (parpool) using the 'local' profile ...
Connected to the parallel pool (number of workers: 4).

ans = 

 Pool with properties: 

            Connected: true
           NumWorkers: 4
              Cluster: local
        AttachedFiles: {}
    AutoAddClientPath: true
          IdleTimeout: 30 minutes (30 minutes remaining)
          SpmdEnabled: true

Evaluating study nsf
Evaluating study bmrk3pain
Evaluating study bmrk3warm
Evaluating study bmrk4
Number of unique values in dataset: 136162681  Bit rate: 27.02 bits
Loaded images:
Direct calls to spm_defauts are deprecated.
Please use spm('Defaults',modality) or spm_get_defaults instead.
Number of unique values in dataset: 129320823  Bit rate: 26.95 bits
Loaded images:
Direct calls to spm_defauts are deprecated.
Please use spm('Defaults',modality) or spm_get_defaults instead.
Number of unique values in dataset: 135864813  Bit rate: 27.02 bits
Number of unique values in dataset: 140850219  Bit rate: 27.07 bits
Loaded images:
Direct calls to spm_defauts are deprecated.
Please use spm('Defaults',modality) or spm_get_defaults instead.
Loaded images:
Direct calls to spm_defauts are deprecated.
Please use spm('Defaults',modality) or spm_get_defaults instead.
Training...training svr kernel linear epsilon=0.1 C=1 optimizer=andre.... 

[Output suppressed, it's long and not interesting]
    
%% RESULTS
% MSE, correlation between predicted and aboserved and run time for cross
% validated predictions are provided below.
%
% Performance metrics are evaluated using a repeated measures statistical
% design. Pearson correlations are z-fisher transformed and compared using
% rm-ANOVA and Tukey post-hoc tests. Because MSE values are skewed, and
% duration may not be normally distributed either, these are evaluated
% using the Friedman test, a nonparametric repeated measures rank test.
% Post-hoc tests on ranks are performed using an exact test (see
% exactfrsd for method citation) which evaluates the likelihood of a rank 
% sum given all combinatorial possibilities of ranks.

%% MSE performance metrics
x = repmat([1,2,3,4],n_studies,1);
y = [d_cverr(:), c_cverr(:), l2_cverr(:), z_cverr(:)];
scales = {'default','centered','normed','zscored'};

[p, ~, stats] = friedman(y,1,'off');
h = barplot_columns(log(y),x,'dolines');
set(gca,'XTick',1:size(y,2),'XTickLabels',scales)
xlabel('Scaling')
ylabel({'log(MSE)','(cross validated)'})
title({'Prediction performace across 15 datasets',sprintf('Friedman p = %0.3f',p)});
a = gca;
a.Title.FontSize = 14;
set(gcf,'Tag','barplot1')

% (post hoc rank test)
pairwise_dif = multcompare(stats,'display','off');
d = pairwise_dif(:,4)*n_studies;
k = size(y,2);
n =  n_studies;
p = zeros(length(d),1);
for i = 1:length(d), p(i) = exactfrsd(d(i),k,n); end

disp('Post-hoc tests');
table(scales(pairwise_dif(:,1))', scales(pairwise_dif(:,2))', pairwise_dif(:,4), p(:),...
    'VariableNames', {'Model_1', 'Model_2', 'Mean_Rank_Difference', 'pValue'})

%% pearson-r performance metrics
r = cellfun(@(x1)(x1.pred_outcome_r),d_stats);
c_r = cellfun(@(x1)(x1.pred_outcome_r),c_stats);
l2_r = cellfun(@(x1)(x1.pred_outcome_r),l2_stats);
z_r = cellfun(@(x1)(x1.pred_outcome_r),z_stats);
x = repmat([1,2,3,4],n_studies,1);
y = [r(:), c_r(:), l2_r(:), z_r(:)];

zr = atanh(y);
t = table(st_datasets',zr(:,1),zr(:,2),zr(:,3),zr(:,4),'VariableNames',{'st','mod1','mod2','mod3','mod4'});
Mod = table(scales','VariableNames',{'Scaling'});
m = fitrm(t,'mod1-mod4 ~ 1','WithinDesign',Mod);
p = m.ranova.pValue;
h = barplot_columns(y,x,'dolines');
set(gca,'XTick',1:size(y,2),'XTickLabels',scales)
xlabel('Scaing')
ylabel({'predicted r','(cross validated)'})
title({'Prediction performace across 15 datasets',sprintf('rm-ANOVA p = %0.3f',p(1))});
a = gca;
a.Title.FontSize = 14;
set(gcf,'Tag','barplot2')

% (post hoc tuckey-test)
disp('Post-hoc tests');
multcompare(m,'Scaling')

%% evaluation time
x = repmat([1,2,3,4],n_studies,1);
y = [d_t,c_t,l2_t,z_t]/60;

[p, ~, stats] = friedman(y,1,'off');
barplot_columns(y,x,'dolines');
set(gca,'XTick',[1,2,3,4],'XTickLabels',scales)
xlabel('Scaling')
ylabel({'Evaluation Time (min)','(5-folds)'})
title({'Compute time across 15 datasets',sprintf('Friedman p = %0.3f',p)});
a = gca;
a.Title.FontSize = 14;
set(gcf,'Tag','barplot3');

% (post hoc rank test)
pairwise_dif = multcompare(stats,'display','off');

d = pairwise_dif(:,4)*n_studies;
k = size(y,2);
n =  n_studies;
for i = 1:length(d), p(i) = exactfrsd(d(i),k,n); end

disp('Post-hoc tests');
table(scales(pairwise_dif(:,1))', scales(pairwise_dif(:,2))', pairwise_dif(:,4), p(:),...
    'VariableNames', {'Model_1', 'Model_2', 'Mean_Rank_Difference', 'pValue'})

%% CONCLUSIONS
% dividing by l2 norm and zscoring scaling procedures lead to improvement 
% in performance as assessed by MSE, the centering effect is
% nonsignificant, and zscoring is significantly better than norming,
% but no methods show significantly better pearson-r based performance
% than any other method. The trends in pearson-r mirror the direction of 
% effects seen for MSE though. 
%
% The improvements in MSE obtained are obtained in significantly less time 
% for zscores than normed or raw data. 
%
% The differences between performance when training on raw data or on 
% zscored data are minor, suggesting results are robust with respect to 
% scaling decisions. Thus, models can be trained and applied to zscored 
% trial maps to facilitate generalization between datasets, with little to 
% no loss of task relevant information, or any real effect on meaningful 
% signal capture whatsoever.
%
% This does not mean that equivalent predictive maps are obtained across
% scaling procedures, or that the information removed is not biologically
% meaningful. However, if this information is meaningful, it appears to not
% be unique, and similar information can be obtained from the scaled
% pattern information. Further investigation is needed to establish whether
% the signal removed by rescaling has biological or task 
% significance.

%% NOTES
% this script was prototyped on quartiled data using the 
% fmri_data_st/quintileByY method. Running on single trials takes too long
% to be practical when develping or debugging code, and this code was only 
% run on single trial data at the very end to generate this output you see
% here.
fprintf('Script runtime (min): %.2f\n',toc(t99)/60);

##### SOURCE END #####
--></body></html>
